{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train MRCNN on Septin\n",
    "##### Braedyn Au 30/8/19\n",
    "\n",
    "Description\n",
    "\n",
    "Within the same folder as this notebook (Mask_RCNN), you will find a folder named Images. In there you will see a *test* and *train* folder. Put your respective images into each folder, ensuring a ratio between 80/20 to 90/10 split between training and testing images.\n",
    "\n",
    "Open Anaconda Powershell Prompt to first use the labelling tool, Labelme. This was installed in a virtual environment as per its developers. First activate this environment\n",
    "\n",
    "    conda activate labelme\n",
    "    \n",
    "You should see the prefix in brackets should change from (base) to (labelme). Then you can run Labelme as a simple command.\n",
    "\n",
    "    labelme\n",
    "    \n",
    "Within the GUI, you must load a directory, either the *test* or *train* folder, and ensure automatic saving is enabled in the options. Using the circle tool (only tool supported currently), draw a circle around each septin ring on each image. These coordinates are saved in a .json file in the same folder as the image. \n",
    "\n",
    "After each image has been labelled, you can continue onto the python code below to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import skimage.draw\n",
    "from os import listdir\n",
    "import tensorflow as tf\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import model as modellib, utils\n",
    "from mrcnn.utils import Dataset\n",
    "from mrcnn.model import MaskRCNN\n",
    "\n",
    "# Directory to save logs and model checkpoints, if not provided\n",
    "# through the command line argument --logs\n",
    "DEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, \"logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning is using prexisting knowledge from other datasets as a starting point to continue training on our data instead of from scratch. Its like instead of teaching a newborn baby how to recognize septin rings, you're teaching a teenager with high school bio knowledge. This is done by loading weights, in this case trained on the COCO dataset which contains real world objects, and adding our objects to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to trained weights file\n",
    "COCO_WEIGHTS_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "The default configuration class can be found in the mrcnn folder in the config.py file. However, although most configuration options are fine being left at default, some should be changed for our training. Instead of changing them in the config.py file, we can create a new class based on the original class with updated parameters.\n",
    "\n",
    "#### Firstly, the STEPS_PER_EPOCH parameter generally should match the number of training samples you have in the train folder.\n",
    "\n",
    "Learning momentum acts as real world momentum on the gradient descent vector, which can speed up time to reach minimum loss but can also cause hill climb, where the loss reaches a minimum but due to momentum begins to climb the gradient and increase loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeptinConfig(Config):\n",
    "    \"\"\"Configuration for training on the septin dataset.\n",
    "    Derives from the base Config class and overrides some values.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"Septin_cfg\"\n",
    "\n",
    "    # We use a GPU with 12GB memory, which can fit two images.\n",
    "    # Adjust down if you use a smaller GPU.\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 1  # Background + Septin\n",
    "\n",
    "    # Number of training steps per epoch\n",
    "    STEPS_PER_EPOCH = 89\n",
    "\n",
    "    # Skip detections with < 90% confidence not used in training\n",
    "    # DETECTION_MIN_CONFIDENCE = 0.5\n",
    "\n",
    "    # Decrease learning momentum\n",
    "    LEARNING_MOMENTUM = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Similar to the config class, there is a default dataset class which can be overwritten to match our dataset. Since our labels are created in Labelme, the labels of rings in each image are stored in json files and thus need to be extracted and put into a mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeptinDataset(Dataset):\n",
    "\n",
    "    def load_Septin(self, subset):\n",
    "        \"\"\"Load a subset of the Septin dataset.\n",
    "        dataset_dir: Root directory of the dataset.\n",
    "        subset: Subset to load: train or test\n",
    "        \"\"\"\n",
    "        # Add classes. We have only one class to add.\n",
    "        self.add_class(\"Septin\", 1, \"Septin\")\n",
    "\n",
    "        # Labels are in same folder as images\n",
    "        assert subset in [\"train\", \"test\"]\n",
    "        dataset_dir =  './Images/'\n",
    "        dataset_dir = os.path.join(dataset_dir, subset)\n",
    "        \n",
    "        for label in listdir(dataset_dir):\n",
    "            if label.endswith('.json'):\n",
    "        # Load annotations\n",
    "                annotations = json.load(open(os.path.join(dataset_dir, label)))\n",
    "                circles = []\n",
    "                regions = [a for a in annotations['shapes']]\n",
    "                # Take the circle coordinates\n",
    "                for a in regions:\n",
    "                    if a['shape_type'] == 'circle':\n",
    "                        c,r = a['points'][0]\n",
    "                        c2,r2 = a['points'][1]\n",
    "                        radius = np.sqrt((c2-c)*(c2-c)+(r2-r)*(r2-r))\n",
    "                        circles.append((int(r),int(c),int(radius)))\n",
    "\n",
    "                image_path = os.path.join(dataset_dir, annotations['imagePath'])\n",
    "                image = skimage.io.imread(image_path)\n",
    "                height, width = image.shape[:2]\n",
    "\n",
    "                self.add_image(\n",
    "                    \"Septin\",\n",
    "                    image_id=annotations['imagePath'],  # use file name as a unique image id\n",
    "                    path=image_path,\n",
    "                    width=width, height=height,\n",
    "                    circles=circles)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for an image.\n",
    "       Returns:\n",
    "        masks: A bool array of shape [height, width, instance count] with\n",
    "            one mask per instance.\n",
    "        class_ids: a 1D array of class IDs of the instance masks.\n",
    "        \"\"\"\n",
    "        # If not a Septin dataset image, delegate to parent class.\n",
    "        image_info = self.image_info[image_id]\n",
    "        if image_info[\"source\"] != \"Septin\":\n",
    "            return super(self.__class__, self).load_mask(image_id)\n",
    "\n",
    "        # Convert circles to a bitmap mask of shape\n",
    "        # [height, width, instance_count]\n",
    "        info = self.image_info[image_id]\n",
    "        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"circles\"])],\n",
    "                        dtype=np.uint8)\n",
    "        for i, p in enumerate(info[\"circles\"]):\n",
    "            # Get indexes of pixels inside the polygon and set them to 1\n",
    "            rr, cc = skimage.draw.circle(p[0], p[1], p[2])\n",
    "            mask[rr, cc, i] = 1\n",
    "\n",
    "        # Return mask, and array of class IDs of each instance. Since we have\n",
    "        # one class ID only, we return an array of 1s\n",
    "        return mask.astype(np.bool), np.ones([mask.shape[-1]], dtype=np.int32)\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the path of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"Septin\":\n",
    "            return info[\"path\"]\n",
    "        else:\n",
    "            super(self.__class__, self).image_reference(image_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "With the config and dataset classes, we can now define a function to train the model. In the final line you will see a model.train() function called. One of the inputs for this is the number of epochs, which means how many times the model looks over all the images. The more epochs the more familiar the model becomes, but too many epochs can lead to overfitting on our training data, which means the model may have a harder time recognizing rings in unseen images. 5-8 epochs is a good range.\n",
    "\n",
    "You may also notice that the layers input is set to heads, which means we are only training the head layers of the neural network, not the entire network. On the Vutara system, the video ram on the graphics card limits us to this as it is much harder to train the whole network. If more vram is available or if this is implemented on the PowerAnalyse, you can set that input to 'all'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config=SeptinConfig()):\n",
    "    \"\"\"Train the model.\"\"\"\n",
    "    # Training dataset.\n",
    "    dataset_train = SeptinDataset()\n",
    "    dataset_train.load_Septin(\"train\")\n",
    "    dataset_train.prepare()\n",
    "    print('Train: %d' % len(dataset_train.image_ids))\n",
    "\n",
    "    # Validation dataset\n",
    "    dataset_val = SeptinDataset()\n",
    "    dataset_val.load_Septin(\"test\")\n",
    "    dataset_val.prepare()\n",
    "    print('Test: %d' % len(dataset_val.image_ids))\n",
    "\n",
    "    config = SeptinConfig()\n",
    "    config.display()\n",
    "\n",
    "    # define the model\n",
    "    model = MaskRCNN(mode='training', model_dir='./', config=config)\n",
    "    # load weights (mscoco) and exclude the output layers\n",
    "    model.load_weights('mask_rcnn_coco.h5', by_name=True, exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\",  \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "    # train weights (output layers or 'heads')\n",
    "    model.train(dataset_train, dataset_val,\n",
    "                learning_rate=config.LEARNING_RATE,\n",
    "                epochs=5,\n",
    "                layers='heads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Check\n",
    "To make sure you are running on the GPU instead of the CPU, run the following line of code. In the Jupyter notebook terminal in your taskbar, you will see which device is being used. If the GPU is configured correctly, something along the likes of \n",
    "    \n",
    "    /job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the Jupyter command prompt shows GPU, you can continue on to the next cell. If it is CPU, the next step will take so so much longer.\n",
    "\n",
    "Double check you have the correct parameters, especially STEPS_PER_EPOCH, number of epochs, etc., and you can call the train function to begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0902 16:44:38.459885 19416 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0902 16:44:38.468886 19416 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0902 16:44:38.505888 19416 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 89\n",
      "Test: 14\n",
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  1024\n",
      "IMAGE_META_SIZE                14\n",
      "IMAGE_MIN_DIM                  800\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "LEARNING_MOMENTUM              0.5\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           Septin_cfg\n",
      "NUM_CLASSES                    2\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                89\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0902 16:44:38.540890 19416 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1919: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0902 16:44:38.544890 19416 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0902 16:44:42.274104 19416 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "W0902 16:44:43.093151 19416 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0902 16:44:43.282161 19416 deprecation_wrapper.py:119] From C:\\Users\\Braedyn Au\\Desktop\\newmaskrcnn\\Mask_RCNN\\mrcnn\\model.py:553: The name tf.random_shuffle is deprecated. Please use tf.random.shuffle instead.\n",
      "\n",
      "W0902 16:44:43.514174 19416 deprecation_wrapper.py:119] From C:\\Users\\Braedyn Au\\Desktop\\newmaskrcnn\\Mask_RCNN\\mrcnn\\utils.py:202: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0902 16:44:43.547177 19416 deprecation.py:506] From C:\\Users\\Braedyn Au\\Desktop\\newmaskrcnn\\Mask_RCNN\\mrcnn\\model.py:600: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: ./septin_cfg20190902T1644\\mask_rcnn_septin_cfg_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0902 16:44:55.539862 19416 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "W0902 16:45:01.818221 19416 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "W0902 16:45:01.819221 19416 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "47/89 [==============>...............] - ETA: 1:16 - loss: 1.5153 - rpn_class_loss: 0.2345 - rpn_bbox_loss: 0.4523 - mrcnn_class_loss: 0.0955 - mrcnn_bbox_loss: 0.3144 - mrcnn_mask_loss: 0.4186"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0902 16:46:30.001265 19416 model.py:1810] Error processing image {'id': '20190809_S5aS2bS5c-g_3-14.tif', 'source': 'Septin', 'path': './Images/train\\\\20190809_S5aS2bS5c-g_3-14.tif', 'width': 998, 'height': 922, 'circles': [(369, 947, 26), (336, 663, 25), (505, 979, 22)]}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Braedyn Au\\Desktop\\newmaskrcnn\\Mask_RCNN\\mrcnn\\model.py\", line 1709, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"C:\\Users\\Braedyn Au\\Desktop\\newmaskrcnn\\Mask_RCNN\\mrcnn\\model.py\", line 1212, in load_image_gt\n",
      "    mask, class_ids = dataset.load_mask(image_id)\n",
      "  File \"<ipython-input-19-461f2828a7ae>\", line 63, in load_mask\n",
      "    mask[rr, cc, i] = 1\n",
      "IndexError: index 998 is out of bounds for axis 1 with size 998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/89 [=========================>....] - ETA: 16s - loss: 1.3216 - rpn_class_loss: 0.1590 - rpn_bbox_loss: 0.3322 - mrcnn_class_loss: 0.0971 - mrcnn_bbox_loss: 0.2920 - mrcnn_mask_loss: 0.4413"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0902 16:47:27.390548 19416 model.py:1810] Error processing image {'id': '20190726_S2aS5b-g_1-2.tif', 'source': 'Septin', 'path': './Images/train\\\\20190726_S2aS5b-g_1-2.tif', 'width': 998, 'height': 1000, 'circles': [(464, 109, 25), (176, 975, 24)]}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Braedyn Au\\Desktop\\newmaskrcnn\\Mask_RCNN\\mrcnn\\model.py\", line 1709, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"C:\\Users\\Braedyn Au\\Desktop\\newmaskrcnn\\Mask_RCNN\\mrcnn\\model.py\", line 1212, in load_image_gt\n",
      "    mask, class_ids = dataset.load_mask(image_id)\n",
      "  File \"<ipython-input-19-461f2828a7ae>\", line 63, in load_mask\n",
      "    mask[rr, cc, i] = 1\n",
      "IndexError: index 998 is out of bounds for axis 1 with size 998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 230s 3s/step - loss: 1.2860 - rpn_class_loss: 0.1483 - rpn_bbox_loss: 0.3112 - mrcnn_class_loss: 0.0983 - mrcnn_bbox_loss: 0.2875 - mrcnn_mask_loss: 0.4407 - val_loss: 1.0683 - val_rpn_class_loss: 0.0720 - val_rpn_bbox_loss: 0.3349 - val_mrcnn_class_loss: 0.1080 - val_mrcnn_bbox_loss: 0.1808 - val_mrcnn_mask_loss: 0.3726\n",
      "Epoch 2/5\n",
      "10/89 [==>...........................] - ETA: 2:16 - loss: 0.8761 - rpn_class_loss: 0.0366 - rpn_bbox_loss: 0.1299 - mrcnn_class_loss: 0.0625 - mrcnn_bbox_loss: 0.2267 - mrcnn_mask_loss: 0.4204"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0902 16:49:58.321180 19416 model.py:1810] Error processing image {'id': '20190726_S2aS5b-g_1-2.tif', 'source': 'Septin', 'path': './Images/train\\\\20190726_S2aS5b-g_1-2.tif', 'width': 998, 'height': 1000, 'circles': [(464, 109, 25), (176, 975, 24)]}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Braedyn Au\\Desktop\\newmaskrcnn\\Mask_RCNN\\mrcnn\\model.py\", line 1709, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"C:\\Users\\Braedyn Au\\Desktop\\newmaskrcnn\\Mask_RCNN\\mrcnn\\model.py\", line 1212, in load_image_gt\n",
      "    mask, class_ids = dataset.load_mask(image_id)\n",
      "  File \"<ipython-input-19-461f2828a7ae>\", line 63, in load_mask\n",
      "    mask[rr, cc, i] = 1\n",
      "IndexError: index 998 is out of bounds for axis 1 with size 998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/89 [====>.........................] - ETA: 2:06 - loss: 0.8664 - rpn_class_loss: 0.0307 - rpn_bbox_loss: 0.1424 - mrcnn_class_loss: 0.0692 - mrcnn_bbox_loss: 0.2150 - mrcnn_mask_loss: 0.4091"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0902 16:50:08.776778 19416 model.py:1810] Error processing image {'id': '20190809_S5aS2bS5c-g_3-14.tif', 'source': 'Septin', 'path': './Images/train\\\\20190809_S5aS2bS5c-g_3-14.tif', 'width': 998, 'height': 922, 'circles': [(369, 947, 26), (336, 663, 25), (505, 979, 22)]}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Braedyn Au\\Desktop\\newmaskrcnn\\Mask_RCNN\\mrcnn\\model.py\", line 1709, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"C:\\Users\\Braedyn Au\\Desktop\\newmaskrcnn\\Mask_RCNN\\mrcnn\\model.py\", line 1212, in load_image_gt\n",
      "    mask, class_ids = dataset.load_mask(image_id)\n",
      "  File \"<ipython-input-19-461f2828a7ae>\", line 63, in load_mask\n",
      "    mask[rr, cc, i] = 1\n",
      "IndexError: index 998 is out of bounds for axis 1 with size 998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 226s 3s/step - loss: 0.9467 - rpn_class_loss: 0.0810 - rpn_bbox_loss: 0.2169 - mrcnn_class_loss: 0.0704 - mrcnn_bbox_loss: 0.1989 - mrcnn_mask_loss: 0.3795 - val_loss: 0.8160 - val_rpn_class_loss: 0.0524 - val_rpn_bbox_loss: 0.2652 - val_mrcnn_class_loss: 0.0725 - val_mrcnn_bbox_loss: 0.1473 - val_mrcnn_mask_loss: 0.2786\n",
      "Epoch 3/5\n",
      "29/89 [========>.....................] - ETA: 1:49 - loss: 0.7901 - rpn_class_loss: 0.0375 - rpn_bbox_loss: 0.1382 - mrcnn_class_loss: 0.0821 - mrcnn_bbox_loss: 0.1966 - mrcnn_mask_loss: 0.3358"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0902 16:54:20.999205 19416 model.py:1810] Error processing image {'id': '20190726_S2aS5b-g_1-2.tif', 'source': 'Septin', 'path': './Images/train\\\\20190726_S2aS5b-g_1-2.tif', 'width': 998, 'height': 1000, 'circles': [(464, 109, 25), (176, 975, 24)]}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Braedyn Au\\Desktop\\newmaskrcnn\\Mask_RCNN\\mrcnn\\model.py\", line 1709, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"C:\\Users\\Braedyn Au\\Desktop\\newmaskrcnn\\Mask_RCNN\\mrcnn\\model.py\", line 1212, in load_image_gt\n",
      "    mask, class_ids = dataset.load_mask(image_id)\n",
      "  File \"<ipython-input-19-461f2828a7ae>\", line 63, in load_mask\n",
      "    mask[rr, cc, i] = 1\n",
      "IndexError: index 998 is out of bounds for axis 1 with size 998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/89 [=================>............] - ETA: 1:03 - loss: 0.8449 - rpn_class_loss: 0.0702 - rpn_bbox_loss: 0.1989 - mrcnn_class_loss: 0.0734 - mrcnn_bbox_loss: 0.1863 - mrcnn_mask_loss: 0.3162"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0902 16:55:06.456805 19416 model.py:1810] Error processing image {'id': '20190809_S5aS2bS5c-g_3-14.tif', 'source': 'Septin', 'path': './Images/train\\\\20190809_S5aS2bS5c-g_3-14.tif', 'width': 998, 'height': 922, 'circles': [(369, 947, 26), (336, 663, 25), (505, 979, 22)]}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Braedyn Au\\Desktop\\newmaskrcnn\\Mask_RCNN\\mrcnn\\model.py\", line 1709, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"C:\\Users\\Braedyn Au\\Desktop\\newmaskrcnn\\Mask_RCNN\\mrcnn\\model.py\", line 1212, in load_image_gt\n",
      "    mask, class_ids = dataset.load_mask(image_id)\n",
      "  File \"<ipython-input-19-461f2828a7ae>\", line 63, in load_mask\n",
      "    mask[rr, cc, i] = 1\n",
      "IndexError: index 998 is out of bounds for axis 1 with size 998\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 998 is out of bounds for axis 1 with size 998",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-671659a046ae>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                 layers='heads')\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\newmaskrcnn\\Mask_RCNN\\mrcnn\\model.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)\u001b[0m\n\u001b[0;32m   2372\u001b[0m             \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2373\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2374\u001b[1;33m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2375\u001b[0m         )\n\u001b[0;32m   2376\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m                 \u001b[0mgenerator_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\newmaskrcnn\\Mask_RCNN\\mrcnn\\model.py\u001b[0m in \u001b[0;36mdata_generator\u001b[1;34m(dataset, config, shuffle, augment, augmentation, random_rois, batch_size, detection_targets, no_augmentation_sources)\u001b[0m\n\u001b[0;32m   1707\u001b[0m                     load_image_gt(dataset, config, image_id, augment=augment,\n\u001b[0;32m   1708\u001b[0m                                 \u001b[0maugmentation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maugmentation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1709\u001b[1;33m                                 use_mini_mask=config.USE_MINI_MASK)\n\u001b[0m\u001b[0;32m   1710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1711\u001b[0m             \u001b[1;31m# Skip images that have no instances. This can happen in cases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\newmaskrcnn\\Mask_RCNN\\mrcnn\\model.py\u001b[0m in \u001b[0;36mload_image_gt\u001b[1;34m(dataset, config, image_id, augment, augmentation, use_mini_mask)\u001b[0m\n\u001b[0;32m   1210\u001b[0m     \u001b[1;31m# Load image and mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1211\u001b[0m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1212\u001b[1;33m     \u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1213\u001b[0m     \u001b[0moriginal_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1214\u001b[0m     image, window, scale, padding, crop = utils.resize_image(\n",
      "\u001b[1;32m<ipython-input-19-461f2828a7ae>\u001b[0m in \u001b[0;36mload_mask\u001b[1;34m(self, image_id)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;31m# Get indexes of pixels inside the polygon and set them to 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0mrr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mskimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcircle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[0mmask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;31m# Return mask, and array of class IDs of each instance. Since we have\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 998 is out of bounds for axis 1 with size 998"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
